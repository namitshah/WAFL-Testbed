from __future__ import annotations

import copy
import csv
import datetime
import json
import logging
import os
import pickle
import random
import socket
import threading
import time
import zlib
from typing import Any, List, Optional, Tuple

from scapy.all import IP, rdpcap


class NetworkTrafficUtils:
    """
    A class for measuring network traffic.
    Responsible for monitoring the P2P port and calculating the total traffic,
    both inbound and outbound, epoch-by-epoch.
    The results are saved to a CSV file.
    """

    last_saved = -1  # The last epoch which was processed.
    last_timestamp = datetime.datetime.min  # The end timestamp of the last epoch.
    addr = "127.0.0.1"
    last_epoch_string = ""  # The last epoch code received from the Control Server.
    logger = None

    def save_traffic_metrics(
        csv_path: str, pcap_path: str, epoch_number: int, neighbours: int, next_last_timestamp: datetime.datetime
    ) -> None:
        """
        Save Network Traffic metrics to the CSV file.
        Tcpdump rotates through a set of 10 5 MB .pcap files.
        This method compiles the network traffic from only those
        files which were modified by tcpdump during the previous
        epoch's duration. The .pcap files are stored in /tmp directory.
        """
        if epoch_number < 0 or NetworkTrafficUtils.last_saved == epoch_number:
            return
        try:
            inbound = 0
            outbound = 0
            packet_list = []
            file_base = os.path.join(pcap_path, "tcpdump")
            for fileID in range(10):
                file_curr = f"{file_base}_{fileID}"
                if os.path.exists(file_curr):
                    mtime = os.path.getmtime(file_curr)
                    if datetime.datetime.fromtimestamp(mtime) < NetworkTrafficUtils.last_timestamp:
                        continue
                    try:
                        packet_list.extend(rdpcap(file_curr))
                    except Exception as e:
                        # No need to track (Files are generated by tcpdump, and
                        # of the set of 10 files, only a few would contain the last
                        # epoch's packet captures).
                        NetworkTrafficUtils.logger.debug(f"Failed to read pcap file '{file_curr}': {e}")
            for pkt in packet_list:
                if (
                    (IP not in pkt)
                    or datetime.datetime.fromtimestamp(float(pkt.time)) < NetworkTrafficUtils.last_timestamp
                    or datetime.datetime.fromtimestamp(float(pkt.time)) > next_last_timestamp
                ):
                    continue
                length = int(pkt.wirelen)
                if pkt[IP].dst == NetworkTrafficUtils.addr:
                    inbound += length
                else:
                    outbound += length
            with open(csv_path, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow([f"{epoch_number}", f"{inbound}", f"{outbound}", f"{neighbours}"])
            # Resetting for next epoch.
            NetworkTrafficUtils.last_saved = epoch_number
            NetworkTrafficUtils.last_timestamp = next_last_timestamp
            NetworkTrafficUtils.logger.info(f"ðŸ“ Traffic Data saved to CSV for epoch {epoch_number}")
        except Exception as e:
            NetworkTrafficUtils.logger.error(f"ðŸ’¥ Failed to save Traffic Data to CSV: {e}")

    def __init__(self, addr: str) -> None:
        """
        Initialize the Network Traffic Utils class.
        All of these are class attributes which must be initialized
        before the network thread is launched.
        """
        NetworkTrafficUtils.last_timestamp = datetime.datetime.now()
        NetworkTrafficUtils.last_saved = -1
        NetworkTrafficUtils.addr = addr
        NetworkTrafficUtils.last_epoch_string = ""
        NetworkTrafficUtils.logger = logging.getLogger("NetworkTrafficUtils")


# Modify this class' methods for WAFL variant implementation.
class ModelLearningUtils:
    """
    A class for the WAFL model learning process.
    ACCESS: in the '../' format (Please run the main.py file from PROJECT/src directory).
    DATASET: PROJECT/dataset/dataset.pickled (Pickle of the subset: torch.utils.data.Dataset);
    MODEL (to be saved as): PROJECT/results/model_instance.pth;
    CONTACT PATTERN: PROJECT/config/contact_pattern.json (Dict of "XXXXX" : [] (epoch: neighbour list));
    CONFIG FILE: PROJECT/config/config_local.json
    """

    def __init__(
        self,
        dataset_path: str,
        model_instance_path: str,
        contact_pattern_path: str,
        model_sharing: ModelSharingUtils,
        ctrl_tcp: CTRL_TCP,
        experiment_id,
        wafl_phase_params: dict,
    ) -> None:
        """
        Initialize the learning process instance.
        Linkages to the train, test, validate dataset sections,
        Initialization of modules (torch, for eg),
        Initialization of the CUDA device, ML model, optimizer,
        Hyperparameter specification, loss method, among other stuff.
        """
        ###
        # Please specify the above here. #
        # Please specify the above here. #
        # Please specify the above here. #
        # Please specify the above here. #
        # Please specify the above here. #
        ###
        self.model_sharing = model_sharing
        self.ctrl_tcp = ctrl_tcp
        self.wafl_phase_params = wafl_phase_params
        self.logger = logging.getLogger("ModelLearningUtils")
        with open(contact_pattern_path, "r") as f:
            self.neighbour_map = json.load(f)
        if not isinstance(self.neighbour_map, list) or len(self.neighbour_map) == 0:
            self.logger.error("Contact pattern must be a non-empty list")
        else:
            self.logger.info(f"Contact pattern loaded: {len(self.neighbour_map)} epochs")
        self.experiment_id = experiment_id
        self.csv_file_path = f"./results/{self.experiment_id}/learning-data.csv"
        self.csv_traf_path = f"./results/{self.experiment_id}/network-data.csv"
        self.pcap_path = "/tmp"
        os.makedirs(os.path.dirname(self.csv_file_path), exist_ok=True)
        with open(self.csv_file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["epoch", "train_acc", "train_loss", "test_acc", "test_loss"])
        os.makedirs(os.path.dirname(self.csv_traf_path), exist_ok=True)
        with open(self.csv_traf_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["epoch", "inbound_bytes", "outbound_bytes", "neighbour_count"])
        self.logger.info(f"ðŸ“Š CSV files created: {self.csv_file_path} and {self.csv_traf_path}")
        self.logger.info("Initialized the Model Learning Utils instance.")
        # Used for the results compilation.
        self.train_loss = 0
        self.train_accuracy = 0
        self.epoch_number = 0

    def get_neighbour_list(self, five_digit_number_str: str = "99999") -> List[int]:
        """
        Return the neighbour list for the current epoch from the contact pattern file.
        """
        neighbour_list = []
        try:
            neighbour_list = self.neighbour_map[int(five_digit_number_str)].get(str(self.model_sharing.agent_index), [])
        except Exception as exc:
            self.logger.error(f"Error retrieving neighbour list: {str(exc)[:100]}...")
        return neighbour_list

    def network_traffic_thread(self, five_digit_number_str: str, sub_command: str, next_last_timestamp: datetime.datetime):
        """
        Compiles and saves the network traffic for the previous epoch
        from the .pcap files generated by the tcpdump process.
        """
        try:
            last_n_nbr = len(self.get_neighbour_list(f"{int(five_digit_number_str) - 1:05d}"))
            # Sub-command: WAFL Learning Stage Code.
            if sub_command == "SELF":
                last_n_nbr = 0
            # Saving the epoch ID for final method call from the kill command segment.
            NetworkTrafficUtils.last_epoch_string = five_digit_number_str
            # Saving the traffic metrics.
            NetworkTrafficUtils.save_traffic_metrics(
                self.csv_traf_path, self.pcap_path, self.epoch_number - 1, last_n_nbr, next_last_timestamp
            )
        except Exception as exc:
            self.logger.error(f"Error saving network traffic: {str(exc)[:100]}...")

    def self_learn(self, five_digit_number_str: str = "99999", WAFL_LEARN=False) -> bool:
        """
        Implementation of the Self-Learning Epoch Logic for WAFL-MLP.
        """
        if not WAFL_LEARN:
            self.logger.info(f"ðŸ§  Beginning the Self-Learning Epoch: {five_digit_number_str}")
        SUCCESS = False
        try:
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            if not WAFL_LEARN:
                self.logger.info(f"ðŸ Completed the Self-Learning Epoch: {five_digit_number_str}")
            # For isolated testing of the Workflow. (MODIFY)
            self.train_accuracy = random.random()
            self.train_loss = random.random()
            self.logger.info(f"ðŸ“‰ Training Loss: {self.train_loss:.6f}")
            self.logger.info(f"ðŸ“ˆ Training Accuracy: {self.train_accuracy:.4f}")
            test_loss, test_accuracy = self._evaluate_model()
            self.logger.info(f"ðŸ“‰ Test Loss: {test_loss:.4f}")
            self.logger.info(f"ðŸ“ˆ Test Accuracy: {test_accuracy:.4f}")
            self._save_results_to_csv(self.epoch_number, self.train_accuracy, self.train_loss, test_accuracy, test_loss)
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            # Code for the Self-TRAIN epoch stage #
            self.epoch_number += 1
            self.model_sharing.update_model_instance(None, "Vanilla")
            SUCCESS = True
        except Exception as exc:
            self.logger.error(f"The following error occurred in self_learn: {str(exc)[:100]}...")
            SUCCESS = False
        return SUCCESS

    def wafl_learn(self, five_digit_number_str: str) -> bool:
        """
        Implementation of the Epoch-by-Epoch Learning Process for WAFL-MLP.
        """
        self.logger.info(f"ðŸŽ¯ Beginning the WAFL-Learning Epoch: {five_digit_number_str}")
        SUCCESS = False
        try:
            neighbours = self.get_neighbour_list(five_digit_number_str)
            n_nbr = len(neighbours)
            local_model = None  # noqa: F841
            for neighbour in neighbours:
                peer_ip = self.ctrl_tcp.get_device_ip(neighbour)
                if peer_ip is None:
                    self.logger.error(f"Could not get IP for neighbour {neighbour}")
                    continue
                received_model = self.model_sharing.request_model_from_peer(peer_ip, "&purpose=Vanilla")  # noqa: F841
                # Code for Model Aggregation #
                # Code for Model Aggregation #
                # Code for Model Aggregation #
                # Code for Model Aggregation #
                # Code for Model Aggregation #
            SELF_LEARN_FLAG = True
            if n_nbr:
                SELF_LEARN_FLAG = self.self_learn(five_digit_number_str, WAFL_LEARN=True)
            else:
                self.logger.info("No model exchange with other agents in this WAFL epoch.")
                test_loss, test_accuracy = self._evaluate_model()
                self.logger.info(f"ðŸ“‰ Training Loss (no exchange): {self.train_loss:.6f}")
                self.logger.info(f"ðŸ“ˆ Training Accuracy (no exchange): {self.train_accuracy:.4f}")
                self.logger.info(f"ðŸ“‰ Test Loss (no exchange): {test_loss:.4f}")
                self.logger.info(f"ðŸ“ˆ Test Accuracy (no exchange): {test_accuracy:.4f}")
                self._save_results_to_csv(self.epoch_number, self.train_accuracy, self.train_loss, test_accuracy, test_loss)
                # Other code for the WAFL-Train epoch stage #
                # Other code for the WAFL-Train epoch stage #
                # Other code for the WAFL-Train epoch stage #
                # Other code for the WAFL-Train epoch stage #
                # Other code for the WAFL-Train epoch stage #
                self.epoch_number += 1
                self.model_sharing.update_model_instance(None, "Vanilla")
            if not SELF_LEARN_FLAG:
                raise Exception("SELF-LEARNING ERROR")
            self.logger.info(f"âœ… Completed the WAFL-Learning Epoch: {five_digit_number_str}")
            SUCCESS = True
        except Exception as exc:
            self.logger.error(f"The following error occurred in wafl_learn: {str(exc)[:100]}...")
            SUCCESS = False
        return SUCCESS

    def _evaluate_model(self) -> Tuple[float, float]:
        """
        Evaluate model on test dataset and return test loss and accuracy.
        """
        # Code for the Model Evaluation Here #
        # Code for the Model Evaluation Here #
        # Code for the Model Evaluation Here #
        # Code for the Model Evaluation Here #
        # Code for the Model Evaluation Here #
        # For isolated testing of the Workflow. (MODIFY)
        accuracy = random.random()
        avg_test_loss = random.random()
        self.logger.info(f"ðŸ“‰ Test Loss: {avg_test_loss:.6f}")
        return avg_test_loss, accuracy

    def _save_results_to_csv(
        self, epoch_str: int, train_accuracy: float, train_loss: float, test_accuracy: float, test_loss: float
    ):
        """
        Save training results to CSV file.
        """
        try:
            with open(self.csv_file_path, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(
                    [epoch_str, f"{train_accuracy:.4f}", f"{train_loss:.6f}", f"{test_accuracy:.4f}", f"{test_loss:.6f}"]
                )
            self.logger.debug(f"ðŸ“ Results saved to CSV for epoch {epoch_str}")
        except Exception as e:
            self.logger.error(f"ðŸ’¥ Failed to save results to CSV: {e}")


class ModelSharingUtils:
    """
    A class for handling peer-to-peer WAFL model sharing.
    """

    cMDLREQ = "MDLREQ"

    def __init__(self, index: int, name: str, addr: str, port: int, timeout: float) -> None:
        """
        Initialize the instance attributes.
        """
        self.vMODEL_INSTANCE = None
        self.vMODEL_INSTANCE_CACHE = None
        self.vMODEL_METADATA = ""
        self.fLISTENER_ACTIVE = True
        self.agent_index = index
        self.name = name
        self.addr = addr
        self.port = port
        self.timeout = timeout
        self.logger = logging.getLogger("ModelSharingUtils")
        self.logger.info("Initialized the Model Sharing Utils instance.")
        self.listener_thread = threading.Thread(target=self._socket_listener_thread, daemon=False, name="P2P_Listener")
        self.listener_thread.start()
        self.logger.info("ðŸš€ Launched the P2P Transfer Thread.")

    def _serialize_model(self, LE_model: Any) -> bytes:
        """
        Serialize the WAFL model for sharing
        (from the last completed epoch).
        """
        self.logger.debug("ðŸ”¢ Serializing the model for transfer.")
        try:
            serialized_output = pickle.dumps(LE_model)
            return serialized_output
        except Exception as exc:
            self.logger.error(f"The following error occurred: {str(exc)[:100]}...")
            return b"ERROR"

    def _deserialize_model(self, SR_model: bytes) -> Any:
        """
        De-serialize the received WAFL model
        (from the last completed epoch).
        """
        self.logger.debug("ðŸ”¢ De-serializing the received model.")
        try:
            deserialized_output = pickle.loads(SR_model)
            return deserialized_output
        except Exception as exc:
            self.logger.error(f"The following error occurred: {str(exc)[:100]}...")
            return b"ERROR"

    def _compress_model(self, LE_model: bytes) -> bytes:
        """
        Lossless compression of the WAFL model for transfer.
        """
        self.logger.info("ðŸ“¦ Compressing the model for transfer.")
        try:
            compressed_output = zlib.compress(LE_model)
            original_size_megabytes = len(LE_model) / 1e6
            compressed_size_megabytes = len(compressed_output) / 1e6
            self.logger.debug(f"ðŸ—œï¸ Compressed from {original_size_megabytes:.2f}MB to {compressed_size_megabytes:.2f}MB.")
            return compressed_output
        except Exception as exc:
            self.logger.error(f"The following error occurred: {str(exc)[:100]}...")
            return b"ERROR"

    def _decompress_model(self, SR_Model: bytes) -> bytes:
        """
        De-compression of the received WAFL model using ZLib.
        """
        self.logger.debug("ðŸ“¦ De-compressing the received model.")
        try:
            decompressed_output = zlib.decompress(SR_Model)
            return decompressed_output
        except Exception as exc:
            self.logger.error(f"The following error occurred: {str(exc)[:100]}...")
            return b"ERROR"

    def _fetch_model(self, peer_IP: str, other_options: str = "") -> Tuple[bool, Any]:
        """
        Implementation of the Model Request (MDLREQ) command.
        Requests the specified peer device for model data.
        other_options attribute, if non-empty, should be prefixed by a '&' character.
        Format of parameters: &param1=val1&param2=val2...
        """
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                self.logger.debug(f"ðŸ“¥ Requesting WAFL model from peer: {str(peer_IP)}")
                command = f"{ModelSharingUtils.cMDLREQ}:src={self.addr}{other_options}\r\n"
                s.settimeout(self.timeout)
                s.connect((peer_IP, self.port))
                s.sendall(command.encode("utf-8"))
                data = []
                while True:
                    packet = s.recv(4096)
                    if not packet:
                        break
                    data.append(packet)
            data = b"".join(data)
            data = self._decompress_model(data)
            data = self._deserialize_model(data)
            if data == b"ERROR" or data is None:
                raise Exception("FETCH ERROR")
            return True, data
        except Exception as exc:
            self.logger.error(f"The following error occurred in _fetch_model: {str(exc)[:100]}...")
            return False, b"ERROR"

    def _dispatch_model(self, conn: socket.socket, options: str) -> bool:
        """
        Utility function for sending the model data to the peer.
        Depending on the WAFL project, the options parameter may
        determine the processing that takes place inside this
        function.
        """
        try:
            self.logger.debug("â³ Preparing the WAFL model data to be dispatched.")
            self.logger.debug(f"ðŸ–¨ï¸ The received OPTIONS for dispatch: {options}")
            # OPTIONS-specific processing
            # code should be added here.
            # For now the entire model is dispatched.
            model_data = self.vMODEL_INSTANCE
            if self.vMODEL_INSTANCE_CACHE is None:
                model_data = self._serialize_model(model_data)
                model_data = self._compress_model(model_data)
                self.vMODEL_INSTANCE_CACHE = model_data
            else:
                model_data = self.vMODEL_INSTANCE_CACHE
            if model_data == b"ERROR":
                self.vMODEL_INSTANCE_CACHE = None
                raise Exception("DISPATCH ERROR")
            conn.sendall(model_data)
            self.logger.debug("âœ… Successfully sent the model data to the peer.")
            return True
        except Exception as exc:
            self.logger.error(f"The following error occurred in _dispatch_model: {str(exc)[:100]}...")
            return False

    def _socket_listener_thread(self) -> None:
        """
        Implemenation of the Peer-to-Peer Sharing Listener Thread.
        Will run as a non-daemon thread for processing MDLREQ requests.
        Will pass on the received OPTIONS to the dispatch model utility.
        Will be run from the __init__() function.
        """
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind((self.addr, self.port))
            s.settimeout(self.timeout)
            s.listen()
            self.logger.info(f"ðŸ”— Socket bound at {self.addr}:{self.port} and listening.")
            while self.fLISTENER_ACTIVE:
                try:
                    conn, addr_info = s.accept()
                    conn.settimeout(self.timeout)
                    self.logger.info(f"ðŸ“¶ Connection Established with {addr_info[0]}:{addr_info[1]}.")
                    data = []
                    while True:
                        packet = conn.recv(4096)
                        if not packet:
                            break
                        data.append(packet)
                        if packet[-2:] == b"\r\n":
                            break
                    data = b"".join(data).decode("utf-8").strip()
                    self.logger.debug(f"'{data}' command received from peer.")
                    command, options = data.split(":")
                    if command != ModelSharingUtils.cMDLREQ:
                        raise Exception("COMMAND MISMATCH")
                    self.logger.info("ðŸ“¡ Dispatching Model Data to the WAFL Peer.")
                    DISPATCHED = self._dispatch_model(conn, options)
                    if not DISPATCHED:
                        raise Exception("NOT DISPATCHED")
                    conn.close()
                except socket.timeout:
                    # This is expected to happen when no connection is made within the timeout.
                    # It allows the while loop to check the fLISTENER_ACTIVE flag.
                    continue
                except Exception as exc:
                    # Avoid logging minor errors that could spam the log.
                    if self.fLISTENER_ACTIVE:
                        self.logger.error(f"The following error occurred in _socket_listener_thread: {str(exc)[:100]}...")
                    time.sleep(1.0)
            self.logger.info("P2P listener thread has been terminated.")

    def update_model_instance(self, LE_model: Any, metadata: str = "") -> None:
        """
        Updates the WAFL model instance that is
        to be dispatched.
        """
        if LE_model is None:
            # For isolated testing of the WAFL Protocol. (MODIFY)
            LE_model = b"MODEL-INSTANCE-DOES-NOT-EXIST" * random.randint(100, 1000)
        self.vMODEL_INSTANCE = copy.deepcopy(LE_model)
        self.vMODEL_INSTANCE_CACHE = None
        self.vMODEL_METADATA = metadata

    def request_model_from_peer(self, peer_IP: str, other_options: str = "") -> Any:
        """
        The wrapper function for retrieving model parameters from a WAFL peer device.
        options attribute, if non-empty, should be prefixed by a '&' character.
        Format of options: &param1=val1&param2=val2...
        Keeps requesing for parameters until they are retrieved successfully.
        Uses Exponential Backoff Mechanism for waiting between retries.
        """
        FETCHED = False
        WAIT_TIME = 2.0
        GROWTH_FACTOR = 1.5
        while not FETCHED and self.fLISTENER_ACTIVE:
            FETCHED, model_data = self._fetch_model(peer_IP, other_options)
            if FETCHED:
                self.logger.info(f"âœ… Retrieved model parameters from peer: {str(peer_IP)}")
                return model_data
            time.sleep(WAIT_TIME)
            WAIT_TIME **= GROWTH_FACTOR
        # Handling KILL command from Control Server.
        return "Fetch Operation Terminated Abruptly."


class CTRL_TCP:
    """
    A class for handling the TCP connection between ctrl server.
    """

    def __init__(self, config_path: str):
        """
        Initialize the TCP connection parameters.
        """
        self.logger = logging.getLogger("CTRL_TCP")
        self.device_names: Optional[List[str]] = None
        self.device_ips: Optional[List[str]] = None
        self.agent_index: Optional[int] = None
        self.name: Optional[str] = None
        self.addr: Optional[str] = None
        self.ctrl_port: Optional[int] = None
        self.p2p_port: Optional[int] = None
        self.timeout: Optional[float] = None
        self.experiment_id: Optional[str] = None

        # Flag to control the main listener loop.
        self.fLISTENER_ACTIVE = True

        # Variables for tracking the current status.
        self.is_epoch_running: bool = False
        self.current_epoch_type: Optional[str] = None  # "SELF" or "WAFL"
        self.current_epoch_number: Optional[str] = None  # 5-digit string
        self.status_logs: List[str] = []

        # Variable to hold the learning thread object.
        self.learning_thread: Optional[threading.Thread] = None
        self.traffic_thread: Optional[threading.Thread] = None

        # Initialize model_sharing and model_learning attributes
        self.model_sharing: Optional[ModelSharingUtils] = None
        self.model_learning: Optional[ModelLearningUtils] = None

        if not self._load_config(config_path):
            raise ValueError(f"Failed to load configuration file | config_path* {config_path}")
        if self.agent_index is None:
            raise ValueError("Device index not properly loaded from configuration")
        if self.name is None:
            raise ValueError("Device name not properly loaded from configuration")

        # Setup the WAFL node before starting the control thread
        self.setup_local_wafl_node(self.agent_index, self.name)
        # Initialize the Network Traffic Utils Class.
        NetworkTrafficUtils(self.addr)
        self.ctrl_listener_thread = threading.Thread(target=self.wait_ctrl, daemon=False, name="CTRL_TCP_Listener")
        self.ctrl_listener_thread.start()
        self.logger.info("Initialized the CTRL_TCP instance with configuration parameters.")

    def _load_config(self, config_path: str) -> bool:
        """
        Loads configuration information from the specified JSON file
        and stores it in member variables.
        Returns:
            bool: True if the configuration was successfully loaded and stored, False otherwise.
        """
        if not os.path.exists(config_path):
            self.logger.error(f"Specified file not found: {config_path}")
            return False

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config_data = json.load(f)
            self.logger.info(f"Configuration file '{config_path}' loaded.")
            experiment_info = config_data.get("experiment_info")
            if not isinstance(experiment_info, dict):
                self.logger.error("Invalid format for 'experiment_info' in JSON. Dictionary required.")
                return False
            self.experiment_id = experiment_info.get("experiment_id")
            if not isinstance(self.experiment_id, str):
                self.logger.error("Invalid format for 'experiment_info.experiment_id' in JSON. String required.")
                return False
            wafl_devices_data = config_data.get("infrastructure")
            if not isinstance(wafl_devices_data, dict):
                self.logger.error("Invalid format for 'wafl_devices' in JSON. Dictionary required.")
                return False
            self.device_names = wafl_devices_data.get("device_names")
            if not (isinstance(self.device_names, list) and all(isinstance(n, str) for n in self.device_names)):
                self.logger.error("Invalid format for 'wafl_devices.name' in JSON. List of strings required.")
                return False
            self.device_ips = wafl_devices_data.get("device_ips")
            if not (isinstance(self.device_ips, list) and all(isinstance(ip, str) for ip in self.device_ips)):
                self.logger.error("Invalid format for 'wafl_devices.ip' in JSON. List of strings required.")
                return False
            agent_info = config_data.get("agent_info")
            if not isinstance(agent_info, dict):
                self.logger.error("Invalid format for 'agent_info' in JSON. Dictionary required.")
                return False
            self.agent_index = agent_info.get("index")
            if not isinstance(self.agent_index, int):
                self.logger.error("Invalid format for 'wafl_devices.index' in JSON. Integer required.")
                return False
            self.name = agent_info.get("device_name")
            if not isinstance(self.name, str):
                self.logger.error("Invalid format for 'wafl_devices.self_name' in JSON. String required.")
                return False
            self.addr = agent_info.get("ip_address")
            if not isinstance(self.addr, str):
                self.logger.error("Invalid format for 'wafl_devices.addr' in JSON. String required.")
                return False
            self.ctrl_port = wafl_devices_data.get("ctrl_port")
            if not isinstance(self.ctrl_port, int):
                self.logger.error("Invalid format for 'wafl_devices.ctrl_port' in JSON. Integer required.")
                return False
            self.p2p_port = wafl_devices_data.get("p2p_port")
            if not isinstance(self.p2p_port, int):
                self.logger.error("Invalid format for 'wafl_devices.p2p_port' in JSON. Integer required.")
                return False
            experiment_parameters = config_data.get("experiment_parameters")
            self.wafl_phase_params = experiment_parameters.get("wafl_phase")
            if not isinstance(self.wafl_phase_params, dict):
                self.logger.error("Invalid format for 'wafl_phase_params' in JSON. Dictionary required.")
                return False
            for key in ["batch_size", "learning_rate", "coefficiency"]:
                if key not in self.wafl_phase_params:
                    raise ValueError(f"Missing required key '{key}' in wafl_phase_params")
            self.timeout = 10.0  # dummy
            if not isinstance(self.timeout, float):
                self.logger.error("Invalid format for 'wafl_devices.timeout' in JSON. Float required.")
                return False
            self.logger.info("All configurations successfully stored in member variables.")
            return True

        except json.JSONDecodeError as e:
            self.logger.error(f"Error parsing JSON file: {e}")
            return False
        except Exception as e:
            self.logger.error(f"An unexpected error occurred while loading configuration: {e}")
            return False

    def get_device_ip(self, agent_index: int) -> Optional[str]:
        """
        Returns the IP address corresponding to the given agent index.
        Returns:
            Optional[str]: The corresponding IP address string, or None if not found.
        """
        if self.device_names is None or self.device_ips is None:
            self.logger.warning("Device names or IPs list is not initialized.")
            return None
        try:
            return self.device_ips[agent_index]
        except IndexError:
            self.logger.warning(
                f"Index for agent {agent_index} is out of bounds for the IP list. Data inconsistency might exist."
            )
            return None

    def setup_local_wafl_node(self, agent_index: int, device_name: str) -> None:
        """
        Sets up the node with the given device name.
        This function should be called before starting the WAFL model training.
        """
        self.logger.info(f"Setting up the node with device name: {device_name}")
        device_ip = self.get_device_ip(agent_index)
        if device_ip is None:
            raise ValueError(f"Could not find IP address for device: {device_name}")
        if self.p2p_port is None:
            raise ValueError("P2P port is not properly configured")
        if self.timeout is None:
            raise ValueError("Timeout is not properly configured")

        model_dir = f"./results/{self.experiment_id}"
        os.makedirs(model_dir, exist_ok=True)
        self.model_sharing = ModelSharingUtils(agent_index, device_name, device_ip, self.p2p_port, self.timeout)
        self.model_learning = ModelLearningUtils(
            "./dataset",
            f"./results/{self.experiment_id}/model_instance",
            "./config/contact_pattern.json",
            self.model_sharing,
            self,
            self.experiment_id,
            self.wafl_phase_params,
        )

    def _receive_command(self, conn: socket.socket) -> Optional[str]:
        """
        Receives a command string from the client connection.
        Returns:
            Optional[str]: The decoded and stripped command string, or None if an error occurs
                           or connection is closed.
        """
        data_buffer = []
        try:
            while True:
                packet = conn.recv(4096)
                if not packet:
                    self.logger.info("Client closed connection.")
                    break
                data_buffer.append(packet)
                if packet.endswith(b"\r\n"):
                    break

            received_command = b"".join(data_buffer).decode("utf-8").strip()
            self.logger.info(f"Received raw command: '{received_command}'")
            return received_command
        except socket.timeout:
            self.logger.warning("Socket receive timed out while waiting for command.")
            return None
        except UnicodeDecodeError:
            self.logger.error("Failed to decode received data as UTF-8. Invalid data format.")
            return None
        except Exception as exc:
            self.logger.error(f"An unexpected error occurred during command reception: {str(exc)[:100]}...")
            return None

    def wait_ctrl(self) -> None:
        """
        Waits for the control server to be ready and processes incoming commands.
        This function should be called before starting the WAFL model training.
        """
        self.logger.info("Waiting for the control server to be ready...")
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind((self.addr, self.ctrl_port))
            s.listen()
            self.logger.info(f"ðŸ”— Socket bound at {self.addr}:{self.ctrl_port} and listening.")

            while self.fLISTENER_ACTIVE:
                try:
                    conn, addr_info = s.accept()
                    conn.settimeout(self.timeout)
                    self.logger.info(f"ðŸ“¶ Connection Established with {addr_info[0]}:{addr_info[1]}.")

                    received_command = self._receive_command(conn)

                    if received_command:
                        if not self._process_command(received_command, conn):
                            self.logger.warning(f"Failed to process command: '{received_command}'")
                    else:
                        self.logger.warning("No valid command received from client or reception failed.")

                    conn.close()
                    self.logger.info(f"Connection with {addr_info[0]}:{addr_info[1]} closed.")

                except socket.timeout:
                    # This is expected behavior to allow the loop to check fLISTENER_ACTIVE.
                    continue
                except Exception as exc:
                    if self.fLISTENER_ACTIVE:
                        self.logger.error(f"An error occurred in the socket listener: {str(exc)[:100]}...")

            self.logger.info("Control listener thread has been terminated.")

    def _process_command(self, command_str: str, conn: socket.socket) -> bool:
        """
        Parses the command string and dispatches to appropriate functions.
        Returns:
            bool: True if the command was successfully processed, False otherwise.
        """
        self.logger.info(f"Processing command: '{command_str}'")
        parts = command_str.split("-")

        main_command = parts[0] if len(parts) > 0 else ""

        try:
            if main_command == "KILL":
                if len(parts) != 1:
                    self.logger.warning(f"Incorrect form of KILL command: {command_str}")
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False
                try:
                    self.traffic_thread = threading.Thread(
                        target=self.model_learning.network_traffic_thread,
                        daemon=False,
                        args=(
                            NetworkTrafficUtils.last_epoch_string,
                            "KILL",
                            datetime.datetime.now(),
                        ),
                    )
                    self.traffic_thread.start()
                except Exception as e:
                    self.logger.error(f"Failed to start traffic thread: {e}")
                # Handle the KILL command by setting flags to terminate loops.
                self.logger.info("KILL command received. Shutting down listeners and ongoing tasks.")
                self.fLISTENER_ACTIVE = False
                if self.model_sharing is not None:
                    self.model_sharing.fLISTENER_ACTIVE = False
                threads_to_join = []
                if self.model_sharing is not None and hasattr(self.model_sharing, "listener_thread"):
                    threads_to_join.append(self.model_sharing.listener_thread)
                if self.learning_thread and self.learning_thread.is_alive():
                    threads_to_join.append(self.learning_thread)
                if self.traffic_thread and self.traffic_thread.is_alive():
                    threads_to_join.append(self.traffic_thread)

                # Wait for threads to finish, with a timeout.
                thread_joined = 0
                for thread in threads_to_join:
                    self.logger.info(f"Waiting for thread {thread.name} to terminate...")
                    thread.join(timeout=self.timeout)
                    if thread.is_alive():
                        self.logger.warning(f"Thread {thread.name} did not terminate in time.")
                    else:
                        thread_joined += 1
                        self.logger.info(f"Thread {thread.name} has terminated successfully.")

                if thread_joined == len(threads_to_join):
                    self.logger.info("All stoppable threads have been processed.")
                    conn.sendall("OK\r\n".encode("utf-8"))
                    return True
                else:
                    self.logger.warning("Not all threads could be stopped. Some might still be running.")
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False

            elif main_command == "STAT":
                if len(parts) != 1:
                    self.logger.warning(f"Incorrect form of STAT command: {command_str}")
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False
                status_response = self._get_status()
                conn.sendall(status_response.encode("utf-8"))
                return True

            elif main_command == "BEGIN":
                if len(parts) != 4:
                    self.logger.warning(
                        f"Incorrect form of BEGIN command: {command_str}. Expected BEGIN-TYPE-NUMBER-ISO_DATETIME."
                    )
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False
                if self.learning_thread and self.learning_thread.is_alive():
                    self.logger.warning("Cannot start new learning task. A task is already running.")
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False

                sub_command = parts[1]
                five_digit_number_str = parts[2]
                epoch_start = datetime.datetime.fromisoformat(parts[3].replace("#", "-"))

                if not (five_digit_number_str.isdigit() and len(five_digit_number_str) == 5):
                    self.logger.warning(f"Invalid five-digit number format in BEGIN command: {command_str}")
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False
                try:
                    self.traffic_thread = threading.Thread(
                        target=self.model_learning.network_traffic_thread,
                        daemon=False,
                        args=(
                            five_digit_number_str,
                            sub_command,
                            epoch_start,
                        ),
                    )
                    self.traffic_thread.start()
                except Exception as e:
                    self.logger.error(f"Failed to start traffic thread: {e}")
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False
                if sub_command == "SELF":
                    try:
                        self.learning_thread = threading.Thread(
                            target=self._self_learn,
                            daemon=False,
                            args=(five_digit_number_str,),
                            name=f"SelfLearn-{five_digit_number_str}",
                        )
                        self.learning_thread.start()
                        conn.sendall("OK\r\n".encode("utf-8"))
                        return True
                    except Exception as e:
                        self.logger.error(f"Failed to start self learning thread: {e}")
                        conn.sendall("ERROR\r\n".encode("utf-8"))
                        return False
                elif sub_command == "WAFL":
                    try:
                        self.learning_thread = threading.Thread(
                            target=self._wafl_learn,
                            daemon=False,
                            args=(five_digit_number_str,),
                            name=f"WAFL_Learn_{five_digit_number_str}",
                        )
                        self.learning_thread.start()
                        conn.sendall("OK\r\n".encode("utf-8"))
                        return True
                    except Exception as e:
                        self.logger.error(f"Failed to start WAFL learning thread: {e}")
                        conn.sendall("ERROR\r\n".encode("utf-8"))
                        return False
                else:
                    self.logger.warning(f"Unknown BEGIN subcommand: {sub_command} in command: {command_str}")
                    conn.sendall("ERROR\r\n".encode("utf-8"))
                    return False

            else:
                self.logger.warning(f"Unknown command received: {command_str}")
                conn.sendall("ERROR\r\n".encode("utf-8"))
                return False
        except Exception as e:
            self.logger.error(f"Error during command processing for '{command_str}': {e}", exc_info=True)
            conn.sendall("ERROR\r\n".encode("utf-8"))
            return False

    def _get_status(self) -> str:
        """
        Constructs the status response string based on the current state.
        Format: EXEC-XXXX-YYYYY-Z or DONE-XXXX-YYYYY-Z
        """
        current_logs = self.status_logs.copy()
        thread_status = "TBD"
        if self.learning_thread:
            thread_status = "Active" if self.learning_thread.is_alive() else "Finished"
        if self.traffic_thread:
            thread_status = "Active" if self.traffic_thread.is_alive() else thread_status
        current_logs.append(f"Learning Thread Status: {thread_status}")
        if self.current_epoch_type is None or self.current_epoch_number is None:
            # Handle case where no epoch has run yet.
            return "DONE-NONE--1-0\r\n"

        log_line_count = len(self.status_logs)

        if self.is_epoch_running or (self.learning_thread and self.learning_thread.is_alive()):
            # Format: EXEC-XXXX-YYYYY-Z
            header = f"EXEC-{self.current_epoch_type}-{self.current_epoch_number}-{log_line_count}"
        else:
            # Format: DONE-XXXX-YYYYY-Z
            header = f"DONE-{self.current_epoch_type}-{self.current_epoch_number}-{log_line_count}"

        # Combine header and logs
        logs = "\n".join(current_logs)
        response = f"{header}\n{logs}\r\n"
        return f"{response}"

    def _self_learn(self, five_digit_number_str: str) -> bool:
        """
        Handles the BEGIN-SELF command.
        """
        self.logger.info(f"Start self learning epoch: {five_digit_number_str}")

        if self.model_learning is None:
            self.logger.error("Model learning is not initialized")
            return False

        # --- Update status at the beginning of the epoch ---
        self.is_epoch_running = True
        self.current_epoch_type = "SELF"
        self.current_epoch_number = five_digit_number_str
        self.status_logs = [f"Log for {self.current_epoch_type} epoch {self.current_epoch_number} started at {time.ctime()}"]
        # ---

        FLAG = self.model_learning.self_learn(five_digit_number_str)
        # --- Update status at the end of the epoch ---
        self.status_logs.append(f"Epoch completion successful: {FLAG} at {time.ctime()}")
        self.is_epoch_running = False
        # ---
        return FLAG

    def _wafl_learn(self, five_digit_number_str: str) -> bool:
        """
        Handles the BEGIN-WAFL command.
        """
        self.logger.info(f"Start WAFL learning epoch: {five_digit_number_str}")

        if self.model_learning is None:
            self.logger.error("Model learning is not initialized")
            return False

        # --- Update status at the beginning of the epoch ---
        self.is_epoch_running = True
        self.current_epoch_type = "WAFL"
        self.current_epoch_number = five_digit_number_str
        self.status_logs = [f"Log for {self.current_epoch_type} epoch {self.current_epoch_number} started at {time.ctime()}"]
        # ---

        # Implement the logic for WAFL learning here
        FLAG = self.model_learning.wafl_learn(five_digit_number_str)

        # --- Update status at the end of the epoch ---
        self.status_logs.append(f"Epoch completion successful: {FLAG} at {time.ctime()}")
        self.is_epoch_running = False
        # ---
        return FLAG


if __name__ == "__main__":
    log_level = os.environ.get("LOG_LEVEL", "INFO").upper()
    level = getattr(logging, log_level, logging.INFO)
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        force=True,
    )
    comm_interface = CTRL_TCP("./config/config.json")
